{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C://Users//Ruhan Siddiqui//Downloads//sentimental analysis\\training.1600000.processed.noemoticon.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('C://Users//Ruhan Siddiqui//Downloads//sentimental analysis'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_encoder=\"ISO-8859-1\"\n",
    "dataset_column=['sentiments','id','date','flag','user','text']\n",
    "dataset=pd.read_csv(\"C://Users//Ruhan Siddiqui//Downloads//sentimental analysis//training.1600000.processed.noemoticon.csv\",encoding=dataset_encoder,names=dataset_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiments</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiments          id                          date      flag  \\\n",
       "0                 0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1                 0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2                 0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3                 0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4                 0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...             ...         ...                           ...       ...   \n",
       "1599995           4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996           4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997           4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998           4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999           4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the unneccesory columns\n",
    "dataset=dataset[['sentiments','text']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiments</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiments                                               text\n",
       "0                 0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1                 0  is upset that he can't update his Facebook by ...\n",
       "2                 0  @Kenichan I dived many times for the ball. Man...\n",
       "3                 0    my whole body feels itchy and like its on fire \n",
       "4                 0  @nationwideclass no, it's not behaving at all....\n",
       "...             ...                                                ...\n",
       "1599995           4  Just woke up. Having no school is the best fee...\n",
       "1599996           4  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997           4  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998           4  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999           4  happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "\n",
       "[1600000 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentiments'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ruhan Siddiqui\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "dataset['sentiments']=dataset['sentiments'].replace(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['sentiments'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiments</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599990</th>\n",
       "      <td>1</td>\n",
       "      <td>WOOOOO! Xbox is back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599991</th>\n",
       "      <td>1</td>\n",
       "      <td>@rmedina @LaTati Mmmm  That sounds absolutely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599992</th>\n",
       "      <td>1</td>\n",
       "      <td>ReCoVeRiNg FrOm ThE lOnG wEeKeNd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599993</th>\n",
       "      <td>1</td>\n",
       "      <td>@SCOOBY_GRITBOYS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>1</td>\n",
       "      <td>@Cliff_Forster Yeah, that does work better tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiments                                               text\n",
       "1599990           1                              WOOOOO! Xbox is back \n",
       "1599991           1  @rmedina @LaTati Mmmm  That sounds absolutely ...\n",
       "1599992           1                  ReCoVeRiNg FrOm ThE lOnG wEeKeNd \n",
       "1599993           1                                  @SCOOBY_GRITBOYS \n",
       "1599994           1  @Cliff_Forster Yeah, that does work better tha...\n",
       "1599995           1  Just woke up. Having no school is the best fee...\n",
       "1599996           1  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599997           1  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599998           1  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599999           1  happy #charitytuesday @theNSPCC @SparksCharity..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, 'Negative'), Text(0, 0, 'Positive')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Ploting\n",
    "plt=dataset.groupby('sentiments').count().plot(kind='bar',title='Data Distribution',legend=True)\n",
    "plt.set_xticklabels([\"Negative\",\"Positive\"],rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the data in list\n",
    "text,sentiment=list(dataset['text']),list(dataset['sentiments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreProcess the data\n",
    "#for preprocess we have to remove all the emojies and stopword and make all letter to small and many more things. Here we gono do the same step by step.\n",
    "\n",
    "# Defining dictionary containing all emojis with their meanings.\n",
    "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n",
    "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
    "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n",
    "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
    "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
    "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n",
    "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
    "# the data i am using here is taken from google you can simply go through this on google and get this much emojies in text form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining set containing all stopwords in english.\n",
    "stopwordlist = ['a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "             'and','any','are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "             'being', 'below', 'between','both', 'by', 'can', 'd', 'did', 'do',\n",
    "             'does', 'doing', 'down', 'during', 'each','few', 'for', 'from', \n",
    "             'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "             'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "             'into','is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "             'me', 'more', 'most','my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "             'only', 'or', 'other', 'our', 'ours','ourselves', 'out', 'own', 're',\n",
    "             's', 'same', 'she', \"shes\", 'should', \"shouldve\",'so', 'some', 'such',\n",
    "             't', 'than', 'that', \"thatll\", 'the', 'their', 'theirs', 'them',\n",
    "             'themselves', 'then', 'there', 'these', 'they', 'this', 'those', \n",
    "             'through', 'to', 'too','under', 'until', 'up', 've', 'very', 'was',\n",
    "             'we', 'were', 'what', 'when', 'where','which','while', 'who', 'whom',\n",
    "             'why', 'will', 'with', 'won', 'y', 'you', \"youd\",\"youll\", \"youre\",\n",
    "             \"youve\", 'your', 'yours', 'yourself', 'yourselves']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(textdata):\n",
    "    processedText = []\n",
    "    \n",
    "    # Create Lemmatizer and Stemmer.\n",
    "    wordLemm = WordNetLemmatizer()\n",
    "    \n",
    "    # Defining regex patterns.\n",
    "    import re\n",
    "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
    "    userPattern       = '@[^\\s]+'\n",
    "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
    "    sequencePattern   = r\"(.)\\1\\1+\"\n",
    "    seqReplacePattern = r\"\\1\\1\"\n",
    "    \n",
    "    for tweet in textdata:\n",
    "        tweet = tweet.lower()\n",
    "        \n",
    "        # Replace all URls with 'URL'\n",
    "        tweet = re.sub(urlPattern,' URL',tweet)\n",
    "        # Replace all emojis.\n",
    "        for emoji in emojis.keys():\n",
    "            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])  \n",
    "        \n",
    "        # Replace @USERNAME to 'USER'.\n",
    "        tweet = re.sub(userPattern,' USER', tweet)        \n",
    "        # Replace all non alphabets.\n",
    "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
    "        # Replace 3 or more consecutive letters by 2 letter.\n",
    "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
    "\n",
    "        tweetwords = ''\n",
    "        for word in tweet.split():\n",
    "            # Checking if the word is a stopword.\n",
    "            #if word not in stopwordlist:\n",
    "            if len(word)>1:\n",
    "                # Lemmatizing the word.\n",
    "                word = wordLemm.lemmatize(word)\n",
    "                tweetwords += (word+' ')\n",
    "            \n",
    "        processedText.append(tweetwords)\n",
    "        \n",
    "    return processedText\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "processedtext = preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(processedtext, sentiment,\n",
    "                                                    test_size = 0.05, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0,\n",
       "                max_features=500000, min_df=1, ngram_range=(1, 2), norm='l2',\n",
       "                preprocessor=None, smooth_idf=True, stop_words=None,\n",
       "                strip_accents=None, sublinear_tf=False,\n",
       "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "vectorization=TfidfVectorizer(ngram_range=(1,2),max_features=500000)\n",
    "vectorization.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorization.transform(X_train)\n",
    "X_test  = vectorization.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data evaluation\n",
    "def model_Evaluate(model):\n",
    "    \n",
    "    # Predict values for Test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    print(cf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "regressor = LogisticRegression(C = 2, max_iter = 1000, n_jobs=-1)\n",
    "regressor.fit(X_train, y_train)\n",
    "model_Evaluate(regressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "file=open('vetorizer.pickle','wb')\n",
    "pickle.dump(vectorization,file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=open('Regressor.pickle','wb')\n",
    "pickle.dump(regressor,file1)\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the Model\n",
    "def load_models():\n",
    "    file=open('./Regressor.pickle','rb')\n",
    "    regression=pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    file=open('./vetorizer.pickle','rb')\n",
    "    vectorizer=pickle.load(file)\n",
    "    file.close()\n",
    "    \n",
    "    return vectorizer,regression\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              text sentiment\n",
      "0                   I hate twitter  Negative\n",
      "1       May the Force be with you.  Positive\n",
      "2  Mr. Stark, I don't feel so good  Negative\n"
     ]
    }
   ],
   "source": [
    "def predict(vectoriser, model, text):\n",
    "    # Predict the sentiment\n",
    "    textdata = vectorization.transform(preprocess(text))\n",
    "    sentiment = model.predict(textdata)\n",
    "    \n",
    "    # Make a list of text with sentiment.\n",
    "    data = []\n",
    "    for text, pred in zip(text, sentiment):\n",
    "        data.append((text,pred))\n",
    "        \n",
    "    # Convert the list into a Pandas DataFrame.\n",
    "    df = pd.DataFrame(data, columns = ['text','sentiment'])\n",
    "    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n",
    "    return df\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Loading the models.\n",
    "    #vectoriser, LRmodel = load_models()\n",
    "    \n",
    "    # Text to classify should be in a list.\n",
    "    text = [\"I hate twitter\",\n",
    "            \"May the Force be with you.\",\n",
    "            \"Mr. Stark, I don't feel so good\"]\n",
    "    \n",
    "    df = predict(vectorization, regressor, text)\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(vectorization,model,text):\n",
    "    textdata=vectorization.transform(preprocess(text))\n",
    "    sentiment=model.predict(textdata)\n",
    "    \n",
    "    data=[]\n",
    "    for text,pred in zip(text,sentiment):\n",
    "        data.append((text,pred))\n",
    "    \n",
    "    #converting list into df\n",
    "    dataset_created=pd.DataFrame(data,columns=['text','sentiment'])\n",
    "    dataset_created= dataset_created.replace([0,1],['Negative','Positive'])\n",
    "    return dataset_created\n",
    "\n",
    "if __name__=='__main__':\n",
    "    text=[\"I Love my India\",\"I am very sad\",\"Good to know You are Fine\",\"See You Soon\",\"Ayush have a Lot Of Patience\",\"I miss mommy  oh, and aba.\",\"rip ravi\",\"wishing warren buffett a very special 90th birthday - thank you for your leadership, friendship unwavening commitment to american innovation\",\"fuck of them\",\"so heartbreaking to read rip chadwick boseman\"]\n",
    "    dataset_Predict=predict(vectorization,regressor,text)\n",
    "    dataset_Predict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_Predict.to_csv(\"Predicted_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I Love my India</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am very sad</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good to know You are Fine</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>See You Soon</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ayush have a Lot Of Patience</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I miss mommy  oh, and aba.</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rip ravi</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>wishing warren buffett a very special 90th bir...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fuck of them</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>so heartbreaking to read rip chadwick boseman</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0                                    I Love my India  Positive\n",
       "1                                      I am very sad  Negative\n",
       "2                          Good to know You are Fine  Positive\n",
       "3                                       See You Soon  Positive\n",
       "4                       Ayush have a Lot Of Patience  Positive\n",
       "5                         I miss mommy  oh, and aba.  Negative\n",
       "6                                           rip ravi  Negative\n",
       "7  wishing warren buffett a very special 90th bir...  Positive\n",
       "8                                       fuck of them  Negative\n",
       "9      so heartbreaking to read rip chadwick boseman  Negative"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
